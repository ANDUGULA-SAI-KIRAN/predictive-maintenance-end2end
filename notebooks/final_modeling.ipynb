{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39de9053",
   "metadata": {},
   "source": [
    "### Scope of Final Modeling Notebook Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5429ed28",
   "metadata": {},
   "source": [
    "This notebook focuses on **consolidation, validation, and final decision-making**, building on insights from prior exploratory and preprocessing analyses.\n",
    "\n",
    "---\n",
    "\n",
    "**Objective**\n",
    "\n",
    "The objectives of this notebook are to:\n",
    "\n",
    "- Summarize key insights from **EDA** and **baseline preprocessing experiments**\n",
    "- Narrow down the modeling scope to **final candidate models and strategies**\n",
    "- Perform a **focused comparison of imbalance-handling approaches**\n",
    "- Select a **final modeling configuration** for production-grade training\n",
    "\n",
    "> This notebook does **not** introduce new feature engineering or hyperparameter tuning.\n",
    "\n",
    "---\n",
    "\n",
    "**Inputs from Previous Analysis**\n",
    "\n",
    "Based on findings from:\n",
    "- `eda.ipynb`\n",
    "- `preprocessing_&_baseline_models.ipynb`\n",
    "\n",
    "The following decisions are treated as **final**:\n",
    "\n",
    "- Feature-augmented (`fe_aug`) representation is the preferred input format\n",
    "- Tree-based ensemble and boosting models are most suitable\n",
    "- Linear and single-tree models are excluded\n",
    "- Model evaluation should prioritize **recall** and **PR-AUC**\n",
    "\n",
    "---\n",
    "\n",
    "**Modeling Scope**\n",
    "\n",
    "This notebook evaluates **final candidate models** under a controlled setup:\n",
    "\n",
    "**Models:**\n",
    "\n",
    "- LightGBM (LGBM)\n",
    "- XGBoost (XGB)\n",
    "- Random Forest (RF)\n",
    "- Decision Tree (DT)\n",
    "\n",
    "**Fixed Components:**\n",
    "\n",
    "- Feature-augmented feature set (`fe_aug`)\n",
    "- Train–test split from mandatory preprocessing\n",
    "- Evaluation metrics: precision, recall, F1-score, PR-AUC\n",
    "\n",
    "---\n",
    "\n",
    "**Focused Imbalance-Handling Comparison**\n",
    "\n",
    "We compare the following configurations using the **feature-augmented dataset**:\n",
    "\n",
    "1. **Feature-augmented + No Imbalance Handling**\n",
    "   - Serves as the reference baseline\n",
    "   - Represents the simplest deployable solution\n",
    "\n",
    "2. **Feature-augmented + Class Weighting**\n",
    "   - Applied selectively to boosting models\n",
    "   - Evaluates recall improvement via algorithm-level weighting\n",
    "\n",
    "3. **Feature-augmented + SMOTE-Tomek**\n",
    "   - Applies data-level resampling on the training set only\n",
    "   - Evaluates recall gains versus false-positive risk\n",
    "\n",
    "**Purpose**\n",
    "- Quantify recall–precision trade-offs\n",
    "- Identify the most balanced and reliable strategy per model\n",
    "- Avoid unnecessary complexity where baseline performance is sufficient\n",
    "\n",
    "---\n",
    "\n",
    "**Summary of Observations**\n",
    "\n",
    "**1️ Baseline Features With Class Weighting**\n",
    "\n",
    "| Model | Weighted | Precision_1 | Recall_1 | F1_1 | PR_AUC |\n",
    "|-------|----------|-------------|----------|------|--------|\n",
    "| XGB_weighted | True | 0.270 | 0.611 | 0.374 | 0.375 |\n",
    "| LGBM_weighted | True | 0.172 | 0.683 | 0.274 | 0.421 |\n",
    "| XGB_normal | False | 0.589 | 0.279 | 0.378 | 0.358 |\n",
    "| LGBM_normal | False | 0.597 | 0.305 | 0.404 | 0.374 |\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "- Class weighting significantly improves recall for boosting models.\n",
    "- Linear models and SVC have poor precision–recall balance.\n",
    "- Boosting with class weighting is effective for minority-class capture but increases false positives.\n",
    "\n",
    "\n",
    "\n",
    "**2️ Domain-Driven Feature Engineering**\n",
    "\n",
    "| Model | Feature Set | Precision_1 | Recall_1 | F1_1 | PR_AUC |\n",
    "|-------|------------|-------------|----------|------|--------|\n",
    "| RF | fe_aug | 0.674 | 0.332 | 0.445 | 0.429 |\n",
    "| LGBM | fe_aug | 0.601 | 0.351 | 0.443 | 0.400 |\n",
    "| RF | fe_only | 0.670 | 0.294 | 0.408 | 0.400 |\n",
    "| LGBM | fe_only | 0.633 | 0.309 | 0.415 | 0.383 |\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "- `fe_aug` consistently outperforms baseline features for tree-based models.\n",
    "- `fe_only` provides competitive performance and reduces feature dimensionality.\n",
    "- Single trees benefit less from engineered features.\n",
    "\n",
    "> **Takeaway:** Feature engineering improves predictive performance, especially when retained **with original features** (`fe_aug`).\n",
    "\n",
    "\n",
    "\n",
    "**3️ Data-Level Handling / Resampling**\n",
    "\n",
    "| Model | Resampling | Precision_1 | Recall_1 | F1_1 | PR_AUC |\n",
    "|-------|------------|-------------|----------|------|--------|\n",
    "| LGBM | SMOTE | 0.265 | 0.607 | 0.369 | 0.374 |\n",
    "| LGBM | SMOTE_Tomek | 0.271 | 0.592 | 0.372 | 0.387 |\n",
    "| XGB | SMOTE | 0.294 | 0.550 | 0.383 | 0.356 |\n",
    "| XGB | SMOTE_Tomek | 0.283 | 0.531 | 0.369 | 0.364 |\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "- Resampling improves recall for minority class.\n",
    "- Precision drops substantially, leading to lower F1 in some cases.\n",
    "- Boosting models still outperform linear or single-tree models even with resampling.\n",
    "\n",
    "> **Takeaway:** Resampling improves recall but introduces false positives; it is **less stable than class weighting or feature augmentation**.\n",
    "\n",
    "---\n",
    "\n",
    "**Consolidated Insights**\n",
    "\n",
    "1. **Feature augmentation (`fe_aug`)** is the most consistent improvement across models.\n",
    "2. **Class weighting** helps **boosting models** achieve higher recall.\n",
    "3. **Data-level resampling** is optional but increases false positives; careful thresholding required.\n",
    "4. **Top candidates for production:**\n",
    "   - `RF_fe_aug`\n",
    "   - `LGBM_fe_aug` (with/without class weighting)\n",
    "   - `XGB_fe_aug` (with class weighting)\n",
    "\n",
    "---\n",
    "**Threshold Optimization**\n",
    "\n",
    "For shortlisted model–strategy combinations:\n",
    "- Precision–Recall curves are analyzed\n",
    "- Decision thresholds are adjusted to align with business risk tolerance\n",
    "- Performance impact is evaluated beyond the default 0.5 threshold\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**\n",
    "By the end of this notebook, we aim to:\n",
    "\n",
    "1. Finalize **1–2 model candidates**.\n",
    "2. Finalize **preferred imbalance-handling strategy**.\n",
    "3. Determine **operating threshold**.\n",
    "4. Handoff to `src/` pipeline:\n",
    "   - Hyperparameter tuning via Optuna\n",
    "   - Experiment tracking and reproducibility with MLflow\n",
    "   - Production-ready implementation\n",
    "\n",
    "---\n",
    "\n",
    "**Out of Scope**\n",
    "\n",
    "- No new feature engineering\n",
    "- No hyperparameter optimization\n",
    "- No automated experiment tracking\n",
    "- Pipeline implementation in production will be handled in the `src/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0829f92e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
