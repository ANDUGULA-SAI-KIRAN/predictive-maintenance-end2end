{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b00efaf",
   "metadata": {},
   "source": [
    "### Dependencies imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934cd822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, precision_recall_curve, auc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00762072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"C:/sai files/projects/predictive-maintenance-end2end/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e031d451",
   "metadata": {},
   "source": [
    "### 1. Preprocessing Strategy and Experiment Design "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a80c7f",
   "metadata": {},
   "source": [
    "This preprocessing phase is designed to **operationalize insights from EDA** and prepare the dataset for systematic machine learning experimentation. The steps below clearly separate **mandatory preprocessing** from **experimental preprocessing**, ensuring clarity, reproducibility, and fair comparison across modeling approaches.\n",
    "\n",
    "---\n",
    "**1. EDA Summary and Preprocessing Rationale**\n",
    "\n",
    "Exploratory analysis established that:\n",
    "- Machine failures are **rare and highly imbalanced**.\n",
    "- Failure events arise from **localized, non-linear operational regimes** driven by feature interactions and thresholds.\n",
    "- Individual failure mode indicators are **not available at inference time** and therefore cannot be used directly for prediction.\n",
    "- Linear correlations between individual features and the failure target are weak, reinforcing the need for **feature engineering and imbalance-aware modeling strategies**.\n",
    "\n",
    "The primary objective of preprocessing is to:\n",
    "- Clean non-informative and leakage-prone columns\n",
    "- Construct a realistic target variable for machine failure prediction\n",
    "- Prepare multiple feature representations to evaluate their impact on downstream model performance\n",
    "- Ensure that scaling, encoding, and feature selection are applied **in a controlled, pipeline-based manner** to avoid data leakage\n",
    "\n",
    "The preprocessing stage acts as the **bridge between EDA and model development**, while aligning all steps with the **primary evaluation metric**, which will focus on metrics suitable for imbalanced classification (e.g., Recall or PR-AUC).\n",
    "\n",
    "---\n",
    "\n",
    "**2. Mandatory Preprocessing (Applied Once, Common to All Approaches)**\n",
    "\n",
    "The following steps are **non-negotiable** and will be applied uniformly before any experimentation:\n",
    "\n",
    "1. **Remove non-informative identifier columns**\n",
    "   - Drop `id` and `Product ID` as they have no predictive value and may introduce leakage.\n",
    "\n",
    "2. **Target variable construction**\n",
    "   - Create a binary target variable `machine_failure`, where:\n",
    "     - `1` if any of `TWF`, `HDF`, `PWF`, `OSF`, or `RNF` equals 1\n",
    "     - `0` otherwise\n",
    "\n",
    "3. **Leakage prevention**\n",
    "   - Drop individual failure mode columns (`TWF`, `HDF`, `PWF`, `OSF`, `RNF`) after target construction to reflect a realistic deployment scenario.\n",
    "\n",
    "4. **Train–test split**\n",
    "   - Perform a stratified split on `machine_failure` to preserve class imbalance.\n",
    "   - Fix the random seed to ensure reproducibility across experiments.\n",
    "\n",
    "5. **Pipeline-aware preprocessing**\n",
    "   - Apply feature scaling **within model pipelines** only when required by the algorithm to avoid leakage.\n",
    "   - Categorical variables such as `Product Type` will be handled consistently via ordinal encoding since it has ordinal relationship.\n",
    "\n",
    "These steps establish a **clean, consistent, and reproducible baseline dataset**.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Experimental Preprocessing Approaches**\n",
    "\n",
    "After mandatory preprocessing, three experimental approaches will be evaluated. Each approach modifies **only one aspect** of the preprocessing or learning setup, enabling fair comparison.\n",
    "\n",
    "---\n",
    "**Approach 1: Baseline Features with Class Weighting**\n",
    "\n",
    "**Objective:**  \n",
    "Evaluate whether handling class imbalance at the **algorithmic level** is effective without altering the feature space.\n",
    "\n",
    "**Planned Steps:**\n",
    "- Use cleaned, original operational features only.\n",
    "- Apply class weighting during model training to penalize misclassification of failure events.\n",
    "- Apply primarily to models that natively support class weighting (tree-based, linear models).\n",
    "\n",
    "This approach isolates the impact of **loss-level imbalance handling**.\n",
    "\n",
    "---\n",
    "\n",
    "**Approach 2: Domain-Driven Feature Engineering**\n",
    "\n",
    "**Objective:**  \n",
    "Assess whether incorporating physically meaningful, interaction-based features improves failure prediction.\n",
    "\n",
    "**Planned Steps:**\n",
    "- Create derived features informed by EDA:\n",
    "  - **Power** from rotational speed and torque\n",
    "  - **Temperature difference** from process and air temperatures\n",
    "- Retain original features alongside engineered features initially.\n",
    "- Optionally evaluate a reduced feature set to assess information compression versus enrichment.\n",
    "- Ensure all engineered features use **only instantaneous sensor values** to avoid temporal or target leakage.\n",
    "- Encode categorical variables consistently within pipelines.\n",
    "\n",
    "This approach focuses on **feature representation** and allows evaluation of **feature impact** independent of imbalance handling.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Approach 3: Data-Level Imbalance Handling (Resampling)**\n",
    "\n",
    "**Objective:**  \n",
    "Examine whether modifying the training data distribution improves failure detection.\n",
    "\n",
    "**Planned Steps:**\n",
    "- Apply resampling techniques such as:\n",
    "  - Synthetic oversampling (e.g., SMOTE)\n",
    "  - Hybrid methods (e.g., SMOTE with Tomek links)\n",
    "- Apply resampling **only on the training set**, keeping the test set unchanged.\n",
    "- Limit resampling experiments to models compatible with synthetic data (primarily tree-based algorithms).\n",
    "\n",
    "This approach isolates the impact of **data-level imbalance handling** while preserving realistic evaluation conditions.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Experimental Consistency and Evaluation Scope**\n",
    "\n",
    "Across all experimental approaches:\n",
    "- The same train–test split will be used.\n",
    "- The same evaluation metrics will be applied.\n",
    "- Differences in performance will be attributed solely to preprocessing and imbalance-handling choices.\n",
    "\n",
    "This controlled setup ensures **interpretable and defensible conclusions**.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Summary**\n",
    "\n",
    "The preprocessing strategy is intentionally structured into:\n",
    "- **Mandatory preprocessing** to ensure data integrity, realism, and reproducibility\n",
    "- **Experimental preprocessing** to systematically evaluate feature engineering and imbalance-handling strategies\n",
    "\n",
    "This design provides a robust foundation for subsequent **model training, tuning, and explainability analysis**, while maintaining clarity for technical reviewers and stakeholders. Final pipeline selection will additionally consider **performance stability, model complexity, and deployment feasibility**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52932c03",
   "metadata": {},
   "source": [
    "### 2. Statistical Validation of Feature–Target Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69198893",
   "metadata": {},
   "source": [
    "#### 2.1 Two-Sample T-Test: Numerical Feature Differences Between Failure and Non-Failure Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1993bff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>T-Statistic</th>\n",
       "      <th>P-Value</th>\n",
       "      <th>Significant?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Air temperature [K]</td>\n",
       "      <td>-21.0085</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Process temperature [K]</td>\n",
       "      <td>-10.8477</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rotational speed [rpm]</td>\n",
       "      <td>18.8057</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Torque [Nm]</td>\n",
       "      <td>-40.6174</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tool wear [min]</td>\n",
       "      <td>-12.7802</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Feature  T-Statistic       P-Value Significant?\n",
       "0      Air temperature [K]     -21.0085  0.0000000000          Yes\n",
       "1  Process temperature [K]     -10.8477  0.0000000000          Yes\n",
       "2   Rotational speed [rpm]      18.8057  0.0000000000          Yes\n",
       "3              Torque [Nm]     -40.6174  0.0000000000          Yes\n",
       "4          Tool wear [min]     -12.7802  0.0000000000          Yes"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Define your numerical features\n",
    "num_features = [\n",
    "    'Air temperature [K]', \n",
    "    'Process temperature [K]', \n",
    "    'Rotational speed [rpm]', \n",
    "    'Torque [Nm]', \n",
    "    'Tool wear [min]'\n",
    "]\n",
    "\n",
    "# 2. Create a list to store the results\n",
    "t_test_results = []\n",
    "\n",
    "failure_cols = ['TWF', 'HDF', 'PWF', 'OSF', 'RNF']\n",
    "df['machine_failure'] = (df[failure_cols].sum(axis=1)>0).astype(int)    \n",
    "\n",
    "for col in num_features:\n",
    "    # Separate the two groups\n",
    "    group_no_failure = df[df['machine_failure'] == 0][col]\n",
    "    group_failure = df[df['machine_failure'] == 1][col]\n",
    "    \n",
    "    # Perform T-Test\n",
    "    t_stat, p_val = stats.ttest_ind(group_no_failure, group_failure)\n",
    "    \n",
    "    # Store results in a dictionary\n",
    "    t_test_results.append({\n",
    "        'Feature': col,\n",
    "        'T-Statistic': round(t_stat, 4),\n",
    "        'P-Value': format(p_val, '.10f'), # Format to show many decimals\n",
    "        'Significant?': 'Yes' if p_val < 0.05 else 'No'\n",
    "    })\n",
    "\n",
    "# 3. Convert to DataFrame for a clean view\n",
    "t_test_df = pd.DataFrame(t_test_results)\n",
    "\n",
    "# Display the result\n",
    "t_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c88f2",
   "metadata": {},
   "source": [
    "#### 2.2 Chi-Square Test of Independence: Product Type vs. Machine Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d17be815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Chi-Square Stat</th>\n",
       "      <th>P-Value</th>\n",
       "      <th>Significant?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Type</td>\n",
       "      <td>22.6564</td>\n",
       "      <td>0.0000120289</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature  Chi-Square Stat       P-Value Significant?\n",
       "0    Type          22.6564  0.0000120289          Yes"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Create the cross-tab\n",
    "contingency_table = pd.crosstab(df['Type'], df['machine_failure'])\n",
    "\n",
    "# 2. Perform the Test\n",
    "chi2, p_val, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# 3. Store in a clean DataFrame (Note the commas at the end of each line!)\n",
    "chi2_results_df = pd.DataFrame({\n",
    "    'Feature': ['Type'],\n",
    "    'Chi-Square Stat': [round(chi2, 4)],\n",
    "    'P-Value': [format(p_val, '.10f')],\n",
    "    'Significant?': ['Yes' if p_val < 0.05 else 'No']\n",
    "})\n",
    "\n",
    "# To view the result in a notebook\n",
    "chi2_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02485ae",
   "metadata": {},
   "source": [
    "To statistically validate the patterns observed during EDA, inferential tests were conducted to assess the relationship between key features and the binary machine failure target.\n",
    "\n",
    "\n",
    "A two-sample t-test was conducted to compare the distributions of key operational features between **failure** and **non-failure** cases. The goal was to assess whether observed differences in feature values are **statistically significant**, not merely visually apparent.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Findings**\n",
    "\n",
    "- **All analyzed features show statistically significant differences** between failure and non-failure groups (p-value < 0.05).\n",
    "- Extremely small p-values (≈ 0) indicate that the observed differences are **highly unlikely to be due to random chance**, given the large sample size.\n",
    "\n",
    "---\n",
    "\n",
    "**Feature-Level Interpretation**\n",
    "\n",
    "- **Torque [Nm]**\n",
    "  - Exhibits the **largest magnitude t-statistic**, indicating the strongest separation between failure and non-failure cases.\n",
    "  - Confirms torque as a **primary driver of machine failure**, consistent with mechanical overstrain and power-related failure regimes.\n",
    "\n",
    "- **Rotational Speed [rpm]**\n",
    "  - Shows a strong and significant difference, with failures occurring at **lower average speeds**.\n",
    "  - Reinforces the importance of **low-speed, high-load operating conditions** in failure scenarios.\n",
    "\n",
    "- **Tool Wear [min]**\n",
    "  - Statistically significant difference supports the role of **accumulated degradation** in triggering failures.\n",
    "  - Aligns with earlier findings on threshold-driven mechanical failures.\n",
    "\n",
    "- **Air Temperature [K] and Process Temperature [K]**\n",
    "  - Both show statistically significant differences, but their **effect sizes are comparatively small**.\n",
    "  - Suggests that while thermal conditions matter, **raw temperature values alone are weak discriminators**, and interaction-based thermal features are more informative.\n",
    "\n",
    "---\n",
    "\n",
    "**Important Interpretation Note**\n",
    "\n",
    "- Statistical significance here reflects **detectable distributional differences**, not predictive strength.\n",
    "- Given the large dataset size, even modest shifts in means can yield very small p-values.\n",
    "\n",
    "---\n",
    "\n",
    "**Overall Insight**\n",
    "\n",
    "The t-test results confirm that **failure and non-failure cases differ systematically across all core operational features**, with the strongest distinctions arising from **mechanical stress and wear-related variables**. These findings validate earlier EDA conclusions and further support the use of **interaction-aware, non-linear models**, rather than relying on individual features or linear assumptions for failure prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f6ff0",
   "metadata": {},
   "source": [
    "### 3. Mandatory Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc256389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['type', 'air_temp', 'process_temp', 'rpm', 'torque', 'tool_wear']\n",
      " Imbalanced data counts train dataset :                  count\n",
      "machine_failure       \n",
      "0                71715\n",
      "1                 1048\n",
      " Imbalanced data counts test dataset :                  count\n",
      "machine_failure       \n",
      "0                17929\n",
      "1                  262\n"
     ]
    }
   ],
   "source": [
    "def mandatory_preprocessing(df: pd.DataFrame, test_size: float = 0.2, random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Perform mandatory preprocessing:\n",
    "    - Drop identifiers, Construct binary target, Remove leakage columns, Stratified train-test split, apply ordinal encoding to type\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Raw dataset including failure columns\n",
    "    test_size : float\n",
    "        Fraction of data for test set\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X_train, X_test, y_train, y_test : pd.DataFrame / pd.Series\n",
    "        Preprocessed train-test split\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Drop non-informative identifiers ---\n",
    "    df = df.drop(columns=['id', 'Product ID'], errors='ignore')\n",
    "    \n",
    "    # --- 2. Create binary target and drop columns ---\n",
    "    failure_cols = ['TWF', 'HDF', 'PWF', 'OSF', 'RNF']\n",
    "    df['machine_failure'] = (df[failure_cols].sum(axis=1)>0).astype(int)    \n",
    "    df = df.drop(columns=failure_cols, errors='ignore')\n",
    "    \n",
    "    # 3. Mapping Low to 0, Medium to 1, and High to 2\n",
    "    quality_mapping = {'L': 0, 'M': 1, 'H': 2}\n",
    "    if 'Type' in df.columns:\n",
    "        df['Type'] = df['Type'].map(quality_mapping)\n",
    "\n",
    "    # 4. rename columns \n",
    "    df.rename(columns={\"Type\":\"type\", \"Air temperature [K]\": \"air_temp\", \"Process temperature [K]\": \"process_temp\", \"Rotational speed [rpm]\": \"rpm\", \n",
    "                       \"Torque [Nm]\": \"torque\", \"Tool wear [min]\": \"tool_wear\"}, inplace=True)\n",
    "\n",
    "    # --- 5. Separate features and target ---\n",
    "    X = df.drop(columns=['machine_failure'])\n",
    "    y = df['machine_failure']\n",
    "    \n",
    "    # --- 6. Stratified train-test split ---\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = mandatory_preprocessing(df)\n",
    "\n",
    "print(f\"columns: {X_train.columns.to_list()}\")\n",
    "print(f\" Imbalanced data counts train dataset : {pd.DataFrame(y_train.value_counts())}\")\n",
    "print(f\" Imbalanced data counts test dataset : {pd.DataFrame(y_test.value_counts())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673d31fa",
   "metadata": {},
   "source": [
    "### 4. Experimental Preprocessing Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956c415",
   "metadata": {},
   "source": [
    "#### 4.1. Base line Features With Class Weighted Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9651a0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {np.int64(0): np.float64(0.5073067001324688), np.int64(1): np.float64(34.715171755725194)}\n",
      "scale_pos_weight: 68.43034351145039\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "scale_pos_weight = class_weight_dict[1] / class_weight_dict[0]\n",
    "\n",
    "print(\"Class weights:\", class_weight_dict)\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00bb2c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    # Decision Tree\n",
    "    \"DT_normal\": DecisionTreeClassifier(random_state=42),\n",
    "    \"DT_weighted\": DecisionTreeClassifier(class_weight=class_weight_dict, random_state=42),\n",
    "\n",
    "    # Random Forest\n",
    "    \"RF_normal\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"RF_weighted\": RandomForestClassifier(\n",
    "        n_estimators=200, class_weight=class_weight_dict, random_state=42\n",
    "    ),\n",
    "\n",
    "    # Logistic Regression\n",
    "    \"LR_normal\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"LR_weighted\": LogisticRegression(\n",
    "        class_weight=class_weight_dict, max_iter=1000, random_state=42\n",
    "    ),\n",
    "\n",
    "    # SVC\n",
    "    \"SVC_normal\": SVC(probability=True, random_state=42),\n",
    "    \"SVC_weighted\": SVC(\n",
    "        class_weight=class_weight_dict, probability=True, random_state=42\n",
    "    ),\n",
    "\n",
    "    # LightGBM\n",
    "    \"LGBM_normal\": LGBMClassifier(random_state=42),\n",
    "    \"LGBM_weighted\": LGBMClassifier(\n",
    "        class_weight=class_weight_dict, random_state=42\n",
    "    ),\n",
    "\n",
    "    # XGBoost\n",
    "    \"XGB_normal\": XGBClassifier(\n",
    "        eval_metric=\"logloss\",\n",
    "        use_label_encoder=False,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"XGB_weighted\": XGBClassifier(\n",
    "        eval_metric=\"logloss\",\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        use_label_encoder=False,\n",
    "        random_state=42\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c038d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: DT_normal\n",
      "Training: DT_weighted\n",
      "Training: RF_normal\n",
      "Training: RF_weighted\n",
      "Training: LR_normal\n",
      "Training: LR_weighted\n",
      "Training: SVC_normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\sai files\\projects\\predictive-maintenance-end2end\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\sai files\\projects\\predictive-maintenance-end2end\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\sai files\\projects\\predictive-maintenance-end2end\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: SVC_weighted\n",
      "Training: LGBM_normal\n",
      "[LightGBM] [Info] Number of positive: 1048, number of negative: 71715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006913 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 932\n",
      "[LightGBM] [Info] Number of data points in the train set: 72763, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014403 -> initscore=-4.225816\n",
      "[LightGBM] [Info] Start training from score -4.225816\n",
      "Training: LGBM_weighted\n",
      "[LightGBM] [Info] Number of positive: 1048, number of negative: 71715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 932\n",
      "[LightGBM] [Info] Number of data points in the train set: 72763, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training: XGB_normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\sai files\\projects\\predictive-maintenance-end2end\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [19:47:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: XGB_weighted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\sai files\\projects\\predictive-maintenance-end2end\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [19:47:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training: {name}\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"weighted\": \"weighted\" in name,\n",
    "        \"precision_1\": report[\"1\"][\"precision\"],\n",
    "        \"recall_1\": report[\"1\"][\"recall\"],\n",
    "        \"f1_1\": report[\"1\"][\"f1-score\"],\n",
    "        \"pr_auc\": pr_auc\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86594887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>weighted</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_1</th>\n",
       "      <th>pr_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGB_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>0.269815</td>\n",
       "      <td>0.610687</td>\n",
       "      <td>0.374269</td>\n",
       "      <td>0.375126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGB_normal</td>\n",
       "      <td>False</td>\n",
       "      <td>0.588710</td>\n",
       "      <td>0.278626</td>\n",
       "      <td>0.378238</td>\n",
       "      <td>0.357883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVC_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>0.055789</td>\n",
       "      <td>0.759542</td>\n",
       "      <td>0.103944</td>\n",
       "      <td>0.108995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVC_normal</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.244275</td>\n",
       "      <td>0.345946</td>\n",
       "      <td>0.359739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RF_normal</td>\n",
       "      <td>False</td>\n",
       "      <td>0.652542</td>\n",
       "      <td>0.293893</td>\n",
       "      <td>0.405263</td>\n",
       "      <td>0.406911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LR_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>0.042834</td>\n",
       "      <td>0.706107</td>\n",
       "      <td>0.080768</td>\n",
       "      <td>0.160869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR_normal</td>\n",
       "      <td>False</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.227324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LGBM_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>0.171620</td>\n",
       "      <td>0.683206</td>\n",
       "      <td>0.274330</td>\n",
       "      <td>0.420789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LGBM_normal</td>\n",
       "      <td>False</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.305344</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.373565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>0.285185</td>\n",
       "      <td>0.293893</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.286292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT_normal</td>\n",
       "      <td>False</td>\n",
       "      <td>0.289037</td>\n",
       "      <td>0.332061</td>\n",
       "      <td>0.309059</td>\n",
       "      <td>0.316499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model  weighted  precision_1  recall_1      f1_1    pr_auc\n",
       "11   XGB_weighted      True     0.269815  0.610687  0.374269  0.375126\n",
       "10     XGB_normal     False     0.588710  0.278626  0.378238  0.357883\n",
       "7    SVC_weighted      True     0.055789  0.759542  0.103944  0.108995\n",
       "6      SVC_normal     False     0.000000  0.000000  0.000000  0.026361\n",
       "3     RF_weighted      True     0.592593  0.244275  0.345946  0.359739\n",
       "2       RF_normal     False     0.652542  0.293893  0.405263  0.406911\n",
       "5     LR_weighted      True     0.042834  0.706107  0.080768  0.160869\n",
       "4       LR_normal     False     0.500000  0.007634  0.015038  0.227324\n",
       "9   LGBM_weighted      True     0.171620  0.683206  0.274330  0.420789\n",
       "8     LGBM_normal     False     0.597015  0.305344  0.404040  0.373565\n",
       "1     DT_weighted      True     0.285185  0.293893  0.289474  0.286292\n",
       "0       DT_normal     False     0.289037  0.332061  0.309059  0.316499"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results).sort_values(\n",
    "    by=['model', 'weighted', \"recall_1\", \"pr_auc\"], ascending=False\n",
    ")\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a854ce",
   "metadata": {},
   "source": [
    "**Experimental Preprocessing: Baseline Features with Class Weighting — Insights & Decisions**\n",
    "\n",
    "This section compares **normal vs. class-weighted training** using the same baseline features to evaluate whether **algorithm-level imbalance handling alone** improves failure prediction.\n",
    "\n",
    "---\n",
    "\n",
    "**Model-Level Observations (Weighted vs. Normal)**\n",
    "\n",
    "- **Decision Tree**\n",
    "  - Normal and weighted versions perform similarly.\n",
    "  - Class weighting does **not provide a clear benefit** for single-tree models.\n",
    "\n",
    "- **Random Forest**\n",
    "  - Normal training performs **better than weighted**, with higher F1-score and PR-AUC.\n",
    "  - Indicates Random Forest already handles imbalance reasonably well without weighting.\n",
    "\n",
    "- **Logistic Regression**\n",
    "  - Class weighting dramatically increases recall but causes **severe precision collapse**.\n",
    "  - Normal version fails to detect failures, while weighted version **overpredicts failures**.\n",
    "\n",
    "- **Support Vector Classifier (SVC)**\n",
    "  - Weighted model achieves very high recall but **near-zero precision**.\n",
    "  - Normal version fails entirely, confirming SVC is **not suitable** for this problem.\n",
    "\n",
    "- **LightGBM**\n",
    "  - Class-weighted version clearly outperforms the normal version.\n",
    "  - Shows strong recall improvement while maintaining competitive PR-AUC.\n",
    "  - Indicates **boosting models benefit from class weighting**.\n",
    "\n",
    "- **XGBoost**\n",
    "  - Class-weighted model significantly improves recall compared to normal training.\n",
    "  - Precision drops, but PR-AUC improves slightly, indicating a **useful trade-off**.\n",
    "\n",
    "---\n",
    "\n",
    "**Overall Observations**\n",
    "\n",
    "- Class weighting **consistently increases recall** across most models.\n",
    "- Precision often degrades sharply, especially for linear and margin-based models.\n",
    "- The effectiveness of class weighting is **highly model-dependent**.\n",
    "- Boosting models handle the recall–precision trade-off **better than other model families**.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusions**\n",
    "\n",
    "- Class-weighted learning **should not be used as a universal solution**.\n",
    "- It is **ineffective or harmful** for linear and SVC-based models.\n",
    "- It is **beneficial for boosting-based models**, where recall gains outweigh precision loss.\n",
    "- Compared to feature engineering, class weighting produces **less stable and less interpretable improvements**.\n",
    "\n",
    "---\n",
    "\n",
    "**Decisions and Future Steps**\n",
    "\n",
    "- **Retain class weighting as an optional strategy**, not a default preprocessing choice.\n",
    "- **Exclude class-weighted linear and SVC models** from further consideration.\n",
    "- **Retain class weighting as an optional enhancement** for boosting models only.\n",
    "- Apply class weighting **on top of feature-augmented features**, not as a standalone strategy.\n",
    "- In future experiments, evaluate whether combining:\n",
    "  - Feature-augmented features\n",
    "  - Selective class weighting  \n",
    "  leads to improved recall without excessive false positives.\n",
    "\n",
    "This experiment confirms that **class weighting is a secondary optimization tool**, not a replacement for robust feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c295d549",
   "metadata": {},
   "source": [
    "#### 4.2 Domain Driven Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "960d845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Function\n",
    "def add_engineered_features(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X_fe = X.copy()\n",
    "    \n",
    "    X_fe[\"power\"] = X_fe[\"rpm\"] * X_fe[\"torque\"]\n",
    "    X_fe[\"temp_diff\"] = X_fe[\"process_temp\"] - X_fe[\"air_temp\"]\n",
    "    X_fe[\"torque_per_rpm\"] = X_fe[\"torque\"] / (X_fe[\"rpm\"] + 1e-6)\n",
    "    \n",
    "    return X_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55b0843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline (no feature engineering)\n",
    "X_train_base = X_train.copy()\n",
    "X_test_base = X_test.copy()\n",
    "\n",
    "# Feature-Augmented\n",
    "X_train_fe_aug = add_engineered_features(X_train)\n",
    "X_test_fe_aug = add_engineered_features(X_test)\n",
    "\n",
    "# Feature-Only\n",
    "independent_cols = [\"type\", \"tool_wear\"]\n",
    "engineered_cols = [\"power\", \"temp_diff\", \"torque_per_rpm\"]\n",
    "\n",
    "X_train_fe_only = X_train_fe_aug[independent_cols + engineered_cols]\n",
    "X_test_fe_only = X_test_fe_aug[independent_cols + engineered_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17b47948",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"DT\": DecisionTreeClassifier(random_state=42),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"LGBM\": LGBMClassifier(random_state=42),\n",
    "    \"XGB\": XGBClassifier(\n",
    "        eval_metric=\"logloss\",\n",
    "        use_label_encoder=False,\n",
    "        random_state=42\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deb479ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Set: baseline ===\n",
      "[LightGBM] [Info] Number of positive: 1048, number of negative: 71715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006407 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 932\n",
      "[LightGBM] [Info] Number of data points in the train set: 72763, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014403 -> initscore=-4.225816\n",
      "[LightGBM] [Info] Start training from score -4.225816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\sai files\\projects\\predictive-maintenance-end2end\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [19:48:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Set: fe_aug ===\n",
      "[LightGBM] [Info] Number of positive: 1048, number of negative: 71715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004902 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1544\n",
      "[LightGBM] [Info] Number of data points in the train set: 72763, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014403 -> initscore=-4.225816\n",
      "[LightGBM] [Info] Start training from score -4.225816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\sai files\\projects\\predictive-maintenance-end2end\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [19:49:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Set: fe_only ===\n",
      "[LightGBM] [Info] Number of positive: 1048, number of negative: 71715\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 72763, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014403 -> initscore=-4.225816\n",
      "[LightGBM] [Info] Start training from score -4.225816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\sai files\\projects\\predictive-maintenance-end2end\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [19:50:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "results_approach2_feature_eng = []\n",
    "\n",
    "feature_sets = {\n",
    "    \"baseline\": (X_train_base, X_test_base),\n",
    "    \"fe_aug\": (X_train_fe_aug, X_test_fe_aug),\n",
    "    \"fe_only\": (X_train_fe_only, X_test_fe_only),\n",
    "}\n",
    "\n",
    "for feature_name, (Xtr, Xte) in feature_sets.items():\n",
    "    print(f\"\\n=== Feature Set: {feature_name} ===\")\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        model.fit(Xtr, y_train)\n",
    "        \n",
    "        y_pred = model.predict(Xte)\n",
    "        y_prob = model.predict_proba(Xte)[:, 1]\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        \n",
    "        results_approach2_feature_eng.append({\n",
    "            \"model\": model_name,\n",
    "            \"feature_set\": feature_name,\n",
    "            \"precision_1\": report[\"1\"][\"precision\"],\n",
    "            \"recall_1\": report[\"1\"][\"recall\"],\n",
    "            \"f1_1\": report[\"1\"][\"f1-score\"],\n",
    "            \"pr_auc\": pr_auc\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2e1c080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>feature_set</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_1</th>\n",
       "      <th>pr_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>fe_aug</td>\n",
       "      <td>0.601307</td>\n",
       "      <td>0.351145</td>\n",
       "      <td>0.443373</td>\n",
       "      <td>0.400084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RF</td>\n",
       "      <td>fe_aug</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.332061</td>\n",
       "      <td>0.445013</td>\n",
       "      <td>0.429231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.289037</td>\n",
       "      <td>0.332061</td>\n",
       "      <td>0.309059</td>\n",
       "      <td>0.316499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DT</td>\n",
       "      <td>fe_only</td>\n",
       "      <td>0.320896</td>\n",
       "      <td>0.328244</td>\n",
       "      <td>0.324528</td>\n",
       "      <td>0.326466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DT</td>\n",
       "      <td>fe_aug</td>\n",
       "      <td>0.294737</td>\n",
       "      <td>0.320611</td>\n",
       "      <td>0.307130</td>\n",
       "      <td>0.310667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGB</td>\n",
       "      <td>fe_only</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.312977</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.378569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>fe_only</td>\n",
       "      <td>0.632812</td>\n",
       "      <td>0.309160</td>\n",
       "      <td>0.415385</td>\n",
       "      <td>0.382623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.305344</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.373565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGB</td>\n",
       "      <td>fe_aug</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.305344</td>\n",
       "      <td>0.402010</td>\n",
       "      <td>0.364917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.652542</td>\n",
       "      <td>0.293893</td>\n",
       "      <td>0.405263</td>\n",
       "      <td>0.406911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RF</td>\n",
       "      <td>fe_only</td>\n",
       "      <td>0.669565</td>\n",
       "      <td>0.293893</td>\n",
       "      <td>0.408488</td>\n",
       "      <td>0.399545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGB</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.588710</td>\n",
       "      <td>0.278626</td>\n",
       "      <td>0.378238</td>\n",
       "      <td>0.357883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model feature_set  precision_1  recall_1      f1_1    pr_auc\n",
       "6   LGBM      fe_aug     0.601307  0.351145  0.443373  0.400084\n",
       "5     RF      fe_aug     0.674419  0.332061  0.445013  0.429231\n",
       "0     DT    baseline     0.289037  0.332061  0.309059  0.316499\n",
       "8     DT     fe_only     0.320896  0.328244  0.324528  0.326466\n",
       "4     DT      fe_aug     0.294737  0.320611  0.307130  0.310667\n",
       "11   XGB     fe_only     0.594203  0.312977  0.410000  0.378569\n",
       "10  LGBM     fe_only     0.632812  0.309160  0.415385  0.382623\n",
       "2   LGBM    baseline     0.597015  0.305344  0.404040  0.373565\n",
       "7    XGB      fe_aug     0.588235  0.305344  0.402010  0.364917\n",
       "1     RF    baseline     0.652542  0.293893  0.405263  0.406911\n",
       "9     RF     fe_only     0.669565  0.293893  0.408488  0.399545\n",
       "3    XGB    baseline     0.588710  0.278626  0.378238  0.357883"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_approach2_feature_eng = pd.DataFrame(\n",
    "    results_approach2_feature_eng\n",
    ").sort_values(\n",
    "    by=[\"recall_1\", \"pr_auc\"], ascending=False\n",
    ")\n",
    "\n",
    "results_approach2_feature_eng\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3730e1e8",
   "metadata": {},
   "source": [
    "**Experimental Preprocessing: Domain-Driven Feature Engineering — Insights & Decisions**\n",
    "\n",
    "This section compares **three feature representations** across models:\n",
    "- **Baseline** (original features)\n",
    "- **Feature-Augmented (fe-aug)** (baseline + engineered interaction features)\n",
    "- **Feature-Only (fe-only)** (engineered features with minimal baseline context)\n",
    "\n",
    "The goal is to understand **which representation consistently improves model performance**.\n",
    "\n",
    "---\n",
    "\n",
    "**Model-wise Comparison:**\n",
    "\n",
    "- **Decision Tree**\n",
    "  - Performance is similar across all feature sets.\n",
    "  - Feature augmentation offers **no clear advantage**, confirming limited model capacity.\n",
    "\n",
    "- **Random Forest**\n",
    "  - **Feature-augmented features perform best**, followed by feature-only, then baseline.\n",
    "  - Indicates that Random Forest benefits from interaction features when combined with baseline inputs.\n",
    "\n",
    "- **LightGBM**\n",
    "  - **Feature-augmented representation clearly performs best**, followed by feature-only, then baseline.\n",
    "  - Confirms LightGBM’s strong ability to exploit engineered interaction features.\n",
    "\n",
    "- **XGBoost**\n",
    "  - **Feature-augmented performs best**, with feature-only next and baseline lowest.\n",
    "  - Shows that XGBoost benefits significantly from engineered features when sufficient context is retained.\n",
    "\n",
    "---\n",
    "\n",
    "**Overall Observations**\n",
    "\n",
    "- **Feature-augmented (fe-aug) consistently outperforms baseline features** across all ensemble and boosting models.\n",
    "- **Feature-only representations improve upon baseline in some cases**, but do not outperform feature-augmented setups.\n",
    "- Single-tree models show minimal benefit from feature engineering.\n",
    "- Improvements achieved through feature engineering are **more balanced and stable** than those observed using class weighting alone.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusions**\n",
    "\n",
    "- **Feature-augmented representations are the most effective feature strategy** identified so far.\n",
    "- Ensemble and boosting models (Random Forest, LightGBM, XGBoost) **benefit the most** from engineered interaction features.\n",
    "- Feature engineering provides **clear and consistent gains**, unlike class weighting, which primarily trades precision for recall.\n",
    "\n",
    "---\n",
    "\n",
    "**Decisions and Future Scope**\n",
    "\n",
    "- **Adopt feature-augmented features as the default input representation** for all future modeling.\n",
    "- Use **class-weighted learning selectively** on top of feature-augmented features, particularly for boosting models.\n",
    "- In future experiments, evaluate whether combining:\n",
    "  - Feature-augmented features\n",
    "  - Selective class weighting\n",
    "  - Data-level resampling  \n",
    "  leads to further improvements in recall without excessive precision loss.\n",
    "\n",
    "This structured comparison confirms that **feature engineering is the strongest lever for improving model performance**, and should form the foundation of the final modeling pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101e8254",
   "metadata": {},
   "source": [
    "#### 4.3 Data level Handling Resampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68f567c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1048, number of negative: 71715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 932\n",
      "[LightGBM] [Info] Number of data points in the train set: 72763, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014403 -> initscore=-4.225816\n",
      "[LightGBM] [Info] Start training from score -4.225816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\sai files\\projects\\predictive-maintenance-end2end\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [19:50:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 71715, number of negative: 71715\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1276\n",
      "[LightGBM] [Info] Number of data points in the train set: 143430, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\sai files\\projects\\predictive-maintenance-end2end\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [19:51:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 71471, number of negative: 71471\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1276\n",
      "[LightGBM] [Info] Number of data points in the train set: 142942, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\sai files\\projects\\predictive-maintenance-end2end\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [19:51:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# --- Define models ---\n",
    "models = {\n",
    "    \"DT\": DecisionTreeClassifier(random_state=42),\n",
    "    \"RF\": RandomForestClassifier(random_state=42),\n",
    "    \"LGBM\": LGBMClassifier(random_state=42),\n",
    "    \"XGB\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# --- Define resampling strategies ---\n",
    "resampling_methods = {\n",
    "    \"baseline\": (X_train, y_train),\n",
    "    \"SMOTE\": SMOTE(random_state=42).fit_resample(X_train, y_train),\n",
    "    \"SMOTE_Tomek\": SMOTETomek(random_state=42).fit_resample(X_train, y_train)\n",
    "}\n",
    "\n",
    "# --- Prepare results dataframe ---\n",
    "results_resampling = []\n",
    "\n",
    "# --- Iterate over resampling methods ---\n",
    "for method_name, (X_res, y_res) in resampling_methods.items():\n",
    "    for model_name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_res, y_res)\n",
    "        # Predict probabilities for PR-AUC\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_probs = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            # For DT or other classifiers without predict_proba fallback to decision_function\n",
    "            y_probs = model.predict(X_test)\n",
    "        # Predict classes\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Metrics for minor class (1)\n",
    "        precision_1 = precision_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "        recall_1 = recall_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "        f1_1 = f1_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "        # PR-AUC\n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_probs)\n",
    "        pr_auc = auc(recall_curve, precision_curve)\n",
    "        # Append results\n",
    "        results_resampling.append({\n",
    "            \"model\": model_name,\n",
    "            \"resampling\": method_name,\n",
    "            \"precision_1\": precision_1,\n",
    "            \"recall_1\": recall_1,\n",
    "            \"f1_1\": f1_1,\n",
    "            \"pr_auc\": pr_auc\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6228ccb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>resampling</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_1</th>\n",
       "      <th>pr_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.293893</td>\n",
       "      <td>0.404199</td>\n",
       "      <td>0.401240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>SMOTE_Tomek</td>\n",
       "      <td>0.270979</td>\n",
       "      <td>0.591603</td>\n",
       "      <td>0.371703</td>\n",
       "      <td>0.386878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.265442</td>\n",
       "      <td>0.606870</td>\n",
       "      <td>0.369338</td>\n",
       "      <td>0.374451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.305344</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.373565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGB</td>\n",
       "      <td>SMOTE_Tomek</td>\n",
       "      <td>0.283096</td>\n",
       "      <td>0.530534</td>\n",
       "      <td>0.369190</td>\n",
       "      <td>0.363796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGB</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.588710</td>\n",
       "      <td>0.278626</td>\n",
       "      <td>0.378238</td>\n",
       "      <td>0.357883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGB</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.293878</td>\n",
       "      <td>0.549618</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.355726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RF</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.287257</td>\n",
       "      <td>0.507634</td>\n",
       "      <td>0.366897</td>\n",
       "      <td>0.334764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RF</td>\n",
       "      <td>SMOTE_Tomek</td>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.515267</td>\n",
       "      <td>0.368852</td>\n",
       "      <td>0.327551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.289037</td>\n",
       "      <td>0.332061</td>\n",
       "      <td>0.309059</td>\n",
       "      <td>0.316499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DT</td>\n",
       "      <td>SMOTE_Tomek</td>\n",
       "      <td>0.134378</td>\n",
       "      <td>0.458015</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.294440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DT</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.124579</td>\n",
       "      <td>0.423664</td>\n",
       "      <td>0.192541</td>\n",
       "      <td>0.273103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model   resampling  precision_1  recall_1      f1_1    pr_auc\n",
       "1     RF     baseline     0.647059  0.293893  0.404199  0.401240\n",
       "10  LGBM  SMOTE_Tomek     0.270979  0.591603  0.371703  0.386878\n",
       "6   LGBM        SMOTE     0.265442  0.606870  0.369338  0.374451\n",
       "2   LGBM     baseline     0.597015  0.305344  0.404040  0.373565\n",
       "11   XGB  SMOTE_Tomek     0.283096  0.530534  0.369190  0.363796\n",
       "3    XGB     baseline     0.588710  0.278626  0.378238  0.357883\n",
       "7    XGB        SMOTE     0.293878  0.549618  0.382979  0.355726\n",
       "5     RF        SMOTE     0.287257  0.507634  0.366897  0.334764\n",
       "9     RF  SMOTE_Tomek     0.287234  0.515267  0.368852  0.327551\n",
       "0     DT     baseline     0.289037  0.332061  0.309059  0.316499\n",
       "8     DT  SMOTE_Tomek     0.134378  0.458015  0.207792  0.294440\n",
       "4     DT        SMOTE     0.124579  0.423664  0.192541  0.273103"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Convert to dataframe ---\n",
    "results_resampling_df = pd.DataFrame(results_resampling)\n",
    "\n",
    "# --- Display sorted results ---\n",
    "results_resampling_df.sort_values(by=[\"pr_auc\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac02051d",
   "metadata": {},
   "source": [
    "**Experimental Preprocessing: Data-Level Imbalance Handling (Resampling) — Insights & Decisions**\n",
    "\n",
    "This experiment evaluates **data-level imbalance handling** by comparing:\n",
    "- **Baseline (no resampling)**\n",
    "- **SMOTE**\n",
    "- **SMOTE + Tomek Links**\n",
    "\n",
    "All models are trained on resampled training data and evaluated on the original (unchanged) test set.\n",
    "\n",
    "---\n",
    "\n",
    "**Model-wise Comparison**\n",
    "\n",
    "- **Decision Tree (DT)**\n",
    "  - Resampling increases recall but **significantly reduces precision and PR-AUC**.\n",
    "  - Baseline performs better overall, indicating DT is **highly sensitive to synthetic samples**.\n",
    "\n",
    "- **Random Forest (RF)**\n",
    "  - Baseline achieves the **best PR-AUC** among RF variants.\n",
    "  - SMOTE and SMOTE-Tomek increase recall but at a **clear cost to precision and PR-AUC**.\n",
    "  - Suggests RF already handles imbalance reasonably well without resampling.\n",
    "\n",
    "- **LightGBM (LGBM)**\n",
    "  - SMOTE and SMOTE-Tomek substantially improve recall compared to baseline.\n",
    "  - PR-AUC remains competitive, though slightly below the baseline RF.\n",
    "  - Indicates **boosting models benefit from controlled resampling**, especially when recall is prioritized.\n",
    "\n",
    "- **XGBoost (XGB)**\n",
    "  - Resampling improves recall over baseline.\n",
    "  - PR-AUC remains comparable but does not exceed baseline performance.\n",
    "  - Shows **moderate benefit**, but gains are not consistently superior.\n",
    "\n",
    "---\n",
    "\n",
    "**Cross-Resampling Observations**\n",
    "\n",
    "- **SMOTE and SMOTE-Tomek consistently increase recall** across all models.\n",
    "- Precision generally decreases, indicating **more false positives**.\n",
    "- **SMOTE-Tomek provides slightly more stable results** than SMOTE alone by reducing noisy synthetic samples.\n",
    "- Resampling benefits are **model-dependent** and strongest for boosting-based models.\n",
    "\n",
    "---\n",
    "\n",
    "**Overall Conclusions**\n",
    "\n",
    "- Data-level resampling is effective for **improving minority-class recall**, but often at the expense of precision.\n",
    "- Tree-based ensemble models tolerate resampling better than single trees.\n",
    "- Compared to class-weighted learning:\n",
    "  - Resampling provides **more balanced improvements** for boosting models.\n",
    "  - But still does not outperform **feature-augmented representations** in terms of stability.\n",
    "\n",
    "---\n",
    "\n",
    "**Decisions and Future Scope**\n",
    "\n",
    "- **Do not apply resampling as a default strategy** across all models.\n",
    "- Prefer **SMOTE or SMOTE-Tomek selectively** with boosting models (LightGBM, XGBoost).\n",
    "- Avoid resampling with single Decision Trees.\n",
    "- Use resampling **only after feature engineering**, not on raw baseline features.\n",
    "- In future experiments, evaluate:\n",
    "  - Feature-augmented features + SMOTE-Tomek\n",
    "  - Feature-augmented features + selective class weighting\n",
    "  - Feature-augmented features + combined resampling and weighting\n",
    "\n",
    "This experiment confirms that **resampling is a powerful but sensitive tool**, best used selectively and in combination with robust feature engineering rather than as a standalone solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4021fc",
   "metadata": {},
   "source": [
    "### 5. Technical summary of preprocessing and baseline models results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2384f9bd",
   "metadata": {},
   "source": [
    "**Project Status Summary**\n",
    "\n",
    "- This project has completed a comprehensive exploratory analysis and preprocessing evaluation to understand machine failure behavior and identify the most effective modeling strategy.\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "- Machine failures are rare and highly imbalanced, requiring specialized handling.\n",
    "\n",
    "- Failures are driven by localized, non-linear interactions between operational variables rather than simple linear trends.\n",
    "\n",
    "- Among multiple preprocessing strategies:\n",
    "\n",
    "    - Domain-driven feature engineering provided the most consistent and stable performance improvements.\n",
    "\n",
    "    - Class weighting and resampling improved failure recall but introduced trade-offs in precision and stability.\n",
    "\n",
    "- Tree-based ensemble and boosting models (Random Forest, LightGBM, XGBoost) consistently outperformed linear and single-tree models.\n",
    "\n",
    "**Strategic Decisions:**\n",
    "\n",
    "- Feature-augmented data representation is selected as the default input for modeling.\n",
    "\n",
    "- LightGBM and XGBoost are identified as primary candidate models.\n",
    "\n",
    "- Imbalance-handling techniques will be applied selectively, not universally, to balance recall and false-alarm risk.\n",
    "\n",
    "**Next Phase Objective:**\n",
    "\n",
    "- Final model training and hyperparameter optimization\n",
    "\n",
    "- Controlled use of imbalance-handling techniques\n",
    "\n",
    "- Model explainability and deployment readiness\n",
    "\n",
    "This structured approach ensures robust, explainable, and production-relevant predictive maintenance models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
