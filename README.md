# ðŸ¤– Predictive Maintenance â€“ End-to-End MLOps Automation

An end-to-end **Predictive Maintenance** system showcasing **Continuous Machine Learning (CML)** and Model inferencing via **FastAPi with Docker Image** for Deployment.

## Technology Stack
> ML: scikit-learn, LightGBM, SHAP, Optuna
> MLOps: DVC, MLflow, Dagshub
> API: FastAPI, Uvicorn
> UI: Streamlit
> DevOps: Docker, Docker Compose
> CML: GitHub Actions
> Data: pandas, numpy, pyarrow, Pydantic validation

---
## Project Structure

project/
â”œâ”€â”€ notebooks/                      # Analysis & experimentation
â”‚   â”œâ”€â”€ 01_eda.ipynb                # Failure patterns & imbalance
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_model_experiments.ipynb
|   â””â”€â”€ Readme.md                   # Notebooks inferences and conlcusions of EDA and preprocessing experiments
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                    # Data pipeline (Phase 1)
â”‚   â”‚   â”œâ”€â”€ data_ingest.py
â”‚   â”‚   â””â”€â”€ preprocess.py
â”‚   â”œâ”€â”€ features/                # Feature engineering (Phase 1)
â”‚   â”‚   â””â”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ models/                  # Model training (Phase 1)
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”‚   â”œâ”€â”€ predict.py
â”‚   â”‚   â””â”€â”€ shap_analysis.py
â”‚   â”œâ”€â”€ api/                     # FastAPI (Phase 2)
â”‚   â”‚   â””â”€â”€ app.py
â”‚   â””â”€â”€ ui/                      # Streamlit (Phase 2)
â”‚       â””â”€â”€ streamlit_ui.py
â”‚
â”œâ”€â”€ data/                        # DVC-tracked datasets
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ feature_engineered/
â”‚
â”œâ”€â”€ models/                      # DVC-tracked models
â”‚   â””â”€â”€ final_model.pkl
â”‚
â”œâ”€â”€ dvc.yaml                     # Pipeline definition (Phase 1)
â”œâ”€â”€ params.yaml                  # Hyperparameter config
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ Dockerfile.fastapi           # API container (Phase 2)
â”œâ”€â”€ Dockerfile.streamlit         # UI container (Phase 2)
â”œâ”€â”€ docker-compose.yml           # Local orchestration (Phase 2)
â””â”€â”€ .github/workflows/           # GitHub Actions (Phase 1)

---

## ðŸ§­ Architectural Overview

The project is divided into **two structured phases**:

- **Phase 1:** Continuous Machine Learning (CI Automation)
- **Phase 2:** Production Deployment (Cloud-Ready)

Phase 1 focuses on automation, reproducibility, and review-time intelligence.  
Phase 2 focuses on model inference using Docker.

---

## ðŸš€ PHASE 1: Continuous Machine Learning (CI Automation)

> When a Pull Request is opened, **GitHub Actions** automatically runs the entire ML lifecycle.

Developer commits changes
        â†“
   Create Pull Request
        â†“
GitHub Actions Triggered (Automatic)
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AUTOMATED ML PIPELINE RUNS         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Data Ingestion (Validate)        â”‚
â”‚ 2. Preprocessing (Clean & Split)    â”‚
â”‚ 3. Feature Engineering              â”‚
â”‚ 4. Hyperparameter Optimization      â”‚
â”‚    â”œâ”€ Random Forest (Optuna: 10 trials) â”‚
â”‚    â””â”€ LightGBM (Optuna: 10 trials)  â”‚
â”‚ 5. Model Training (5-fold CV)       â”‚
â”‚ 6. Evaluation & Metrics             â”‚
â”‚ 7. SHAP Analysis                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
  âœ… Pipeline Complete
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RESULTS POSTED IN PULL REQUEST     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ðŸ“Š Performance Metrics              â”‚
â”‚    â€¢ Recall | F1 | PR-AUC           â”‚
â”‚    â€¢ Confusion Matrix (Visual)      â”‚
â”‚ ðŸ“ˆ Model Comparison (RF vs LightGBM)â”‚
â”‚ ðŸŽ¯ Threshold Optimization Results   â”‚
â”‚ ðŸ” SHAP Explainability Charts       â”‚
â”‚ âœ“ Pass/Fail Validation              â”‚
â”‚    (Recall â‰¥ 0.7, F1 â‰¥ 0.6)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
  Decision: Merge or Request Changes


### Step 1: Developer Creates a Pull Request

```bash
git checkout -b feature/improve-model
# Make changes to code or parameters
git push origin feature/improve-model
# Open Pull Request on GitHub
```

### Step 2: GitHub Actions Auto-Triggers

Workflow trigger configuration:

``` 
on:
  pull_request:
    branches:
      - main
```
âœ… Instant: Triggered automatically on PR creation
âœ… No Manual Intervention: Zero waiting time
âœ… Isolated Environment: Runs on fresh GitHub runners (clean state)


### Step 3: ML Pipeline Executes (Automated)
DVC Pipeline stages run sequentially:

Stage 1: INGEST
â””â”€ Command: python -m src.data.data_ingest
   Output: data/raw/combined.csv

Stage 2: PREPROCESS
â””â”€ Command: python -m src.data.preprocess
   Inputs: data/raw/combined.csv
   Outputs: 
      - data/processed/train.csv (80%)
      - data/processed/test.csv (20%)

Stage 3: FEATURE ENGINEERING
â””â”€ Command: python -m src.features.feature_engineering
   Inputs: train.csv, test.csv
   Outputs:
      - data/feature_engineered/train_enriched.csv
      - data/feature_engineered/test_enriched.csv

Stage 4: HYPERPARAMETER OPTIMIZATION & TRAINING
â””â”€ Command: python -m src.models.train
   
   ðŸ”„ MODEL 1: RANDOM FOREST
   â”œâ”€ Optuna Search: 10 trials Ã— 5-fold CV
   â”œâ”€ Hyperparameters tuned:
   â”‚  â”œâ”€ n_estimators: [100, 500]
   â”‚  â”œâ”€ max_depth: [5, 30]
   â”‚  â”œâ”€ min_samples_split: [2, 20]
   â”‚  â”œâ”€ min_samples_leaf: [1, 10]
   â”‚  â”œâ”€ max_features: ["sqrt", "log2", 0.3-0.8]
   â”‚  â””â”€ class_weight: ["balanced", null]
   â””â”€ Best Model: Saved & Logged to MLflow
   
   ðŸ”„ MODEL 2: LIGHTGBM
   â”œâ”€ Optuna Search: 10 trials Ã— 5-fold CV
   â”œâ”€ Hyperparameters tuned:
   â”‚  â”œâ”€ n_estimators: [100, 500]
   â”‚  â”œâ”€ max_depth: [5, 30]
   â”‚  â”œâ”€ learning_rate: [0.01, 0.2]
   â”‚  â”œâ”€ num_leaves: [20, 150]
   â”‚  â”œâ”€ min_child_samples: [5, 50]
   â”‚  â”œâ”€ subsample & colsample_bytree: [0.6-1.0]
   â”‚  â””â”€ class_weight: ["balanced", null]
   â””â”€ Best Model: Saved & Logged to MLflow

Stage 5: MODEL EVALUATION
â””â”€ Command: python -m src.models.evaluate
   Metrics calculated:
   â”œâ”€ Precision, Recall, F1-Score
   â”œâ”€ PR-AUC, ROC-AUC
   â”œâ”€ Confusion Matrix
   â””â”€ Threshold optimization analysis

Stage 6: EXPLAINABILITY ANALYSIS
â””â”€ Command: python -m src.models.shap_analysis
   Generate:
   â”œâ”€ SHAP Summary Plots
   â”œâ”€ Feature Importance Rankings
   â””â”€ Decision explanations


### Step 4: Results Automatically Posted to PR (via CML)
Continuous Machine Learning (CML) Bot Posts:
```
 Random Forest (Baseline)
- Recall: 0.72 
- F1-Score: 0.68 
- Precision: 0.65
- ROC-AUC: 0.79

 LightGBM (Class-Weighted)
- Recall: 0.78 âœ…
- F1-Score: 0.75 âœ…
- Precision: 0.72
- ROC-AUC: 0.81
```

### Step 5: Decision Point

The reviewer evaluates the Pull Request based on **automated, objective metrics** â€” no manual model evaluation is required.

**What the reviewer sees in the PR:**
- âœ… Automated evaluation metrics generated by the CI pipeline

**Decision Logic:**
- The newly trained modelâ€™s performance is **compared against the current production model**
  stored in **DagsHubâ€“MLflow**.
- Approval is granted **only if the new model outperforms the production model**
  on the defined evaluation metrics (e.g., Recall, F1-score, PR-AUC).

**Actions:**
- âœ… **APPROVE & MERGE**  
  â†’ New model outperforms production  
  â†’ Model is **registered/promoted to production** in MLflow
- âŒ **REQUEST CHANGES**  
  â†’ New model underperforms production  
  â†’ Developer adjusts parameters or code and the pipeline **re-runs automatically**

---
### ðŸ”„ Continuous Automation Benefits

| What Automates | Before (Manual) | After (Phase 1) |
|---------------|------------------|----------------|
| Model Training | Run locally, ~30 minutes, hope it works | Auto-triggered, isolated, reproducible |
| Hyperparameter Tuning | Manual grid search, days of work | Optuna auto-optimizes in minutes |
| Metrics Calculation | Manual notebooks, error-prone | Auto-computed and validated |
| Results Reporting | Screenshots in Slack, unclear | Professional report posted in PR |
| Threshold Decisions | Subjective and inconsistent | Data-driven, automatic validation |
| Model Registry | Manual upload, version confusion | Auto-registered if validation passes |
| Code Review | â€œLooks good to meâ€ (no data context) | Reviewer sees actual impact with metrics |
| Deployment Decision | Guesswork, high risk | Clear pass/fail criteria |

---

### 1. Clone & Setup (~5 min)
git clone https://github.com/ANDUGULA-SAI-KIRAN/predictive-maintenance-end2end.git
cd predictive-maintenance-end2end

### 2. Make Changes (2 min)
#### Edit params.yaml to tune hyperparameters
params.yaml
#### OR modify src/models/train.py logic

### 3. Commit & Push (1 min)
git add params.yaml src/models/train.py
git commit -m "Improve model: increase LightGBM learning rate"
git push origin feature/tune-lgbm

### 4. Create PR (1 min)
#### Go to GitHub, click "Create Pull Request"
#### Add description: "Testing new hyperparameters for LightGBM or RF"

### 5. WAIT & WATCH (GitHub does all the work now) â³
#### GitHub Actions automatically:
âœ… Spins up runner
âœ… Installs dependencies
âœ… Runs complete ML pipeline
âœ… Calculates metrics
âœ… Generates visualizations
âœ… Posts results in PR comments

### 6. Review Results (3 min)
#### Check PR comments for metrics & charts
#### If metrics look good â†’ MERGE
#### If not â†’ Make changes & repeat
---

### Phase 1 Key Automations
- Trigger: PR creation â†’ GitHub Actions fires
- Data Pipeline: Ingest â†’ Preprocess â†’ Feature Engineer (auto)
- Model Training: Random Forest + LightGBM with Optuna (auto)
- Evaluation: Metrics calculated on test set (auto)
- Reporting: CML posts results to PR (auto)
- Validation: Pass/Fail check against thresholds (auto)
- Registration: Models logged to MLflow if pass (auto)
> Result: Developer changes code â†’ Results appear in PR within minutes. No manual ML operations needed.

---

## PHASE 2: PRODUCTION DEPLOYMENT (Cloud-Ready)

Once Phase 1 âœ… passes and model is updated as production in MLflow(Dagshub) stores the model weights:
```
Model in MLflow Registry
        â†“
Docker Containers Built (FastAPI + Streamlit)
        â†“
Deploy to Cloud (AWS SageMaker / Azure ML)
        â†“
Real-time Inference (Automated scaling)
```

> Note: Refer Readme_docker.md for phase 2 implementations

## Conclusion
This project goal is to showcases a robust architecture for continuously integrating machine learning models with GitHub Actions while preparing for seamless deployment in cloud environments. The focus on automation and reliability is key to ensuring machine learning models are always up to date and ready for production use.
